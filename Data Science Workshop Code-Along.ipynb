{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science - Introduction to Sentiment Analysis Workshop\n",
    "\n",
    "This workshop will cover a very important and interesting field in data science: sentiment analysis! Sentiment analysis has limitless applications. It is used in customer support analysis, market research, brand monitoring, etc. The big question is: how can we extract useful information and insights from seemingly plain text? \n",
    "\n",
    "### What we'll cover\n",
    "- What is sentiment analysis? Why sentiment analysis?\n",
    "- Getting started (importing libraries, dataset)\n",
    "- Data preprocessing (cleaning, transforming)\n",
    "- Classification Modeling\n",
    "- Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "\n",
    "Let's start by importing all necessary libraries. The main ones we'll need are pandas, nltk, and scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #helps us view, store, and process our data\n",
    "import nltk #helpful NLP-specific functions and libraries\n",
    "import sklearn #helps us setup, train and evaluate our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings adjustment so we can see the full text within the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)  # or 199"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's import our dataset\n",
    "\n",
    "Today, we'll be predicting sentiments of IMDB Movie reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our dataset looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Data preprocessing is an important step for any data science task. In order to make our data useable for our model (your computer), we need to go through a few data cleaning/transforming steps. \n",
    "\n",
    "First, let's important any important data processing module from our libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a RegexpTokenizer. This will help us get rid of any special characters (#, $, &, etc.) in our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initiate a CountVectorizer, passing in our tokenizer from earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data and count vectorizer all set up, let's tokenize our data! This effectively converts our text data into a matrix of integer data based on the frequency of each word in each review, which is interpretable by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step in the data preprocessing step is splitting our data into training and test sets. We can use a very helpful function from scikit-learn to accomplish this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into training and testing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Now that we have our data all preprocessed and split, we are ready to start modeling! \n",
    "\n",
    "We can start by importing and instantiating an instance of our model. Today, we'll be using the __modelname__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#type here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our model and data all prepared, we can start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#type here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Now that our model is all trained, how do we see how good it is? Let's first import the metrics module from sklearn, which will help us evaluate the results of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caluclating the accuracy score of the model\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now obtain predictions from our model. Remember, the model has never seen this data, so the results should be a good indicator of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our predictions, we can compare them with the true values and see how well we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Score: \",accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "We have our well-performing model, but how do we actually use this model in a practical sense? In other words, how can we efficiently input text and receive sentiments from them? \n",
    "\n",
    "Let's create a function to help us with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrediction(text, cv, model):\n",
    "    '''\n",
    "    Takes in a sequence of texts, the pre-fit CountVectorizer, trained model and returns the models predictions\n",
    "    on the texts in the form of a dataframe.\n",
    "    '''\n",
    "    textCounts = cv.transform(text)\n",
    "    predictions = model.predict(textCounts)\n",
    "    sentiments = list(map(lambda x: \"Positive\" if x == 1 else \"Negative\", predictions))\n",
    "    #return a df\n",
    "    return pd.DataFrame({\"text\": text, \"predictions\": sentiments})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out our function! Let's say we're building a movie rating website (like IMDB), and we have user-input reviews.\n",
    "\n",
    "Try it yourself! Find an IMDB review for your favorite movie. **Make sure you can detect the sentiment yourself**. Set the text of the review equal to the review variable below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"REPLACE THIS WITH A REVIEW FROM YOUR FAVORITE MOVIE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getPrediction([review], cv, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's it\n",
    "\n",
    "Congratulations! You build your own sentiment classifier!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
